% Manuscript: Options as Basis Functions: A Unifying Perspective
\documentclass[11pt]{article}
\usepackage{amsmath,amsthm,amssymb,mathtools}
\usepackage{graphicx, float}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage[capitalize]{cleveref}
\geometry{margin=1in}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\DeclareMathOperator{\spanop}{span}

\title{Options as Basis Functions: A Unifying Perspective Connecting Finance, Splines, and Neural Networks}
\author{Henry James Ramstad}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present an expository synthesis showing that European call option payoffs 
$(x-K)_+ = \max(x-K,0)$ share identical mathematical structure with linear 
B-splines from approximation theory and ReLU activation functions from neural 
networks. While the density of linear combinations of such functions in $C([a,b])$ 
is known separately in each field, we provide a unified exposition that makes 
the connections explicit. We include constructive proofs via piecewise-linear 
approximation, error bounds for Hölder-continuous functions, and the standard 
integral representation for $C^2$ functions. Selected results are formally 
verified in Lean~4 accompanied by a full software implementation for practical payoff decomposition and analysis. This synthesis highlights opportunities for cross-disciplinary 
transfer of techniques between mathematical finance, approximation theory, and 
machine learning. 
\end{abstract}

\section{Introduction}

For a strike $K \in \mathbb{R}$, the European call option payoff is $(x-K)_+ := \max(x-K,0)$. This simple piecewise-linear function appears under different names across multiple disciplines: in finance as a call option payoff, in approximation theory as a linear B-spline or hinge function, and in machine learning as a shifted ReLU activation. 

\textbf{Contribution.} While the density of linear combinations of such functions in $C([a,b])$ is known in each field separately, we provide a unified exposition that makes the connections explicit. Our treatment includes:
\begin{enumerate}
    \item Constructive proofs using only elementary real analysis
    \item Quantitative error bounds for different smoothness classes
    \item Formal verification of the integral representation in Lean~4
    \item Explicit identification of the mathematical equivalence: $\text{option payoff} = \text{linear B-spline} = \text{ReLU activation}$
\end{enumerate}

\textbf{Note on novelty}: The individual mathematical results presented here 
are not new; they appear in standard references in each field. Our contribution 
is expository: we make the connections between these fields explicit and 
provide a unified presentation that may be useful for researchers working at 
their intersections.

\textbf{Significance.} This unification suggests that techniques from one field may be applicable in others. For instance, optimal strike placement for option portfolios relates to optimal knot placement in spline approximation and optimal bias initialization in neural networks.

\subsection{Historical Context and Related Work}

The mathematical properties of functions $(x-t)_+$ have been studied independently in several fields:

\begin{itemize}
\item \textbf{Spline Theory}: Linear B-splines of the form $(x-t_i)_+$ form a basis for piecewise-linear functions (de Boor, 2001). Our Lemma~\ref{lem:spline_repr} appears as Theorem 1.1 in de Boor's text.

\item \textbf{Neural Networks}: Leshno et al. (1993) proved that neural networks with activation $\sigma(x)=\max(x,0)$ (ReLU) are universal approximators. Our Theorem~\ref{thm:density} provides an alternative constructive proof.

\item \textbf{Financial Mathematics}: Breeden and Litzenberger (1978) showed state prices can be recovered from option prices, while Carr and Madan (2001) developed replication strategies using options. Our work provides the approximation-theoretic foundation for these approaches.
\end{itemize}

Our contribution is to synthesize these separate results into a coherent framework, highlighting their mathematical unity.

\subsection{Organization}

Section~\ref{sec:preliminaries} establishes notation. Section~\ref{sec:density} proves density constructively. Section~\ref{sec:rates} provides error bounds. Section~\ref{sec:integral} presents the integral representation. Section~\ref{sec:connections} discusses interdisciplinary connections. Section~\ref{sec:verification} describes Lean formalization. Section~\ref{sec:conclusion} concludes with future directions.

\section{Preliminaries}\label{sec:preliminaries}

Let $[a,b] \subset \mathbb{R}$ be a compact interval with $a < b$.

\begin{definition}[Function spaces]
$C([a,b])$ denotes continuous real-valued functions on $[a,b]$ with supremum norm $\|f\|_\infty = \sup_{x\in[a,b]}|f(x)|$. $C^k([a,b])$ denotes $k$-times continuously differentiable functions. $C^{0,\alpha}([a,b])$ denotes Hölder continuous functions of order $\alpha \in (0,1]$.
\end{definition}

\begin{definition}[Hinge function]
For $K \in \mathbb{R}$, the \textbf{hinge function} is $\phi_K(x) := (x-K)_+ = \max(x-K,0)$.
\end{definition}

This function has three equivalent interpretations:
\begin{itemize}
    \item \textbf{Financial}: Payoff of European call option with strike $K$
    \item \textbf{Approximation}: Linear B-spline (basis function for piecewise-linear approximation)
    \item \textbf{Machine Learning}: Shifted ReLU activation: $\phi_K(x) = \text{ReLU}(x-K)$
\end{itemize}

\begin{definition}[Option span]
The \textbf{option span} is the set of finite linear combinations:
\[
\mathcal{S} := \spanop\{1, x, \phi_K : K \in [a,b]\}
= \left\{\alpha + \beta x + \sum_{i=1}^N w_i \phi_{K_i}(x) : N \in \mathbb{N}, \alpha,\beta,w_i \in \mathbb{R}, K_i \in [a,b]\right\}.
\]
\end{definition}

\section{Density of the Option Span}\label{sec:density}

The following results are known in various forms across different fields. We present unified constructive proofs.

\begin{theorem}[Density of $\mathcal{S}$ in $C([a,b])$]\label{thm:density}
For every $f \in C([a,b])$ and $\varepsilon > 0$, there exist $N \in \mathbb{N}$, strikes $K_1,\dots,K_N \in [a,b]$, and coefficients $\alpha,\beta,w_1,\dots,w_N \in \mathbb{R}$ such that
\[
\sup_{x\in[a,b]} \left| f(x) - \left( \alpha + \beta x + \sum_{i=1}^N w_i (x-K_i)_+ \right) \right| < \varepsilon.
\]
\end{theorem}

\begin{proof}[Proof sketch]
The proof proceeds in two steps:

1. \textbf{Piecewise-linear approximation}: By uniform continuity of $f$, there exists a piecewise-linear function $s$ (linear spline) with $\|f-s\|_\infty < \varepsilon$.

2. \textbf{Spline representation}: Any piecewise-linear function $s$ with knots $a=x_0 < x_1 < \cdots < x_n=b$ can be written as:
\[
s(x) = \alpha + \beta x + \sum_{j=1}^{n-1} \gamma_j (x-x_j)_+
\]
where $\alpha = s(a) - m_1 a$, $\beta = m_1$, and $\gamma_j = m_{j+1} - m_j$ for slopes $m_j = (s(x_j)-s(x_{j-1}))/(x_j-x_{j-1})$.

Combining these steps yields the result. Full details are standard in spline theory.
\end{proof}

\begin{remark}
This theorem appears in spline theory as the density of linear B-splines, in neural network theory as the universal approximation theorem for ReLU networks (Leshno et al., 1993), and provides mathematical foundation for option-based replication in finance.
\end{remark}

\section{Approximation Rates}\label{sec:rates}

While density guarantees approximation in the limit, quantitative rates depend on function smoothness.

\begin{theorem}[Error bound for Hölder functions]\label{thm:holder_rate}
Let $f \in C^{0,\alpha}([a,b])$ satisfy $|f(x)-f(y)| \leq L|x-y|^\alpha$ for some $L>0$, $\alpha \in (0,1]$. For any $n \in \mathbb{N}$, there exists $s_n \in \mathcal{S}$ using at most $n$ interior knots such that
\[
\|f - s_n\|_\infty \leq 2L h^\alpha,
\]
where $h = (b-a)/n$ is the mesh size of a uniform partition.
\end{theorem}

\begin{proof}
Take uniform partition $x_j = a + jh$, $j=0,\dots,n$. Let $s_n$ be the piecewise-linear interpolant satisfying $s_n(x_j) = f(x_j)$. For $x \in [x_{j-1}, x_j]$, write $x = \theta x_j + (1-\theta)x_{j-1}$, $\theta \in [0,1]$. Then
\[
|f(x)-s_n(x)| \leq |f(x)-f(x_{j-1})| + \theta|f(x_{j-1})-f(x_j)| \leq Lh^\alpha + Lh^\alpha = 2Lh^\alpha.
\]
\end{proof}

\begin{remark}
For Lipschitz functions ($\alpha=1$), the error is $O(n^{-1})$. Smoother functions achieve better rates; for $f \in C^2([a,b])$, one can achieve $O(n^{-2})$ through the integral representation below.
\end{remark}

\section{Integral Representation}\label{sec:integral}

For twice-differentiable functions, an exact representation provides additional insight.

\begin{proposition}[Integral representation]\label{prop:integral_repr}
Let $f \in C^2([a,b])$. Then for all $x \in [a,b]$,
\[
f(x) = f(a) + f'(a)(x-a) + \int_a^x (x-t) f''(t) \, dt.
\]
\end{proposition}

\begin{proof}
Define $g(x) = f(a) + f'(a)(x-a) + \int_a^x (x-t) f''(t) \, dt$. Differentiating and applying the Fundamental Theorem of Calculus gives $g'(x) = f'(x)$. Since $g(a) = f(a)$, we have $g \equiv f$.
\end{proof}

This representation shows $f''$ acts as a density for hinge functions, since $(x-t) = \int_t^x ds$ and $(x-t)_+$ appears naturally when $x > t$.

\begin{remark}[Formal verification]
Proposition~\ref{prop:integral_repr} has been formally verified in Lean~4 using Mathlib's interval integral library. The 72-line proof handles all technical details of differentiation under the integral sign and application of the Fundamental Theorem of Calculus.
\end{remark}

\begin{corollary}[Riemann approximation]
For any partition $a = t_0 < t_1 < \cdots < t_n = b$, define
\[
f_n(x) = f(a) + f'(a)(x-a) + \sum_{i=1}^{n-1} w_i (x-t_i)_+,
\]
where $w_i = f''(\xi_i)(t_i - t_{i-1})$ for some $\xi_i \in (t_{i-1}, t_i)$. Then as $\max_i (t_i - t_{i-1}) \to 0$, we have $\|f - f_n\|_\infty \to 0$.
\end{corollary}

\section{Interdisciplinary Connections}\label{sec:connections}

\subsection{Equivalence of Representations}

The mathematical identity $\phi_K(x) = (x-K)_+ = \text{ReLU}(x-K)$ establishes:

\begin{itemize}
\item \textbf{Finance to ML}: Any ReLU network $g(x) = \alpha + \beta x + \sum_{i=1}^N w_i \text{ReLU}(x-K_i)$ corresponds exactly to an option portfolio with cash $\alpha$, stock position $\beta$, and $w_i$ options at strikes $K_i$.

\item \textbf{ML to Approximation}: ReLU network training corresponds to finding optimal weights (and potentially strikes) for function approximation.

\item \textbf{Approximation to Finance}: Optimal knot placement in spline approximation corresponds to optimal strike selection for option-based replication.
\end{itemize}

\subsection{Interpretations of Training/Learning}

\begin{itemize}
\item \textbf{Neural Network Training}: Learning weights $w_i$ and biases $K_i$ to minimize $\|f - g\|$

\item \textbf{Option Portfolio Optimization}: Selecting strikes $K_i$ and notionals $w_i$ to replicate payoff $f$

\item \textbf{Spline Approximation}: Choosing knots $K_i$ and coefficients $w_i$ for piecewise-linear interpolation
\end{itemize}

These are mathematically identical problems, suggesting potential for cross-pollination of techniques.

\section{Formal Verification}\label{sec:verification}
We have verified selected results in 
Lean~4 using Mathlib. While these verifications do not constitute mathematical 
novelty, they demonstrate the feasibility of formalizing financial mathematics 
concepts and provide rigorous checking of reasoning about integrals and derivatives.

The verified results include the integral representation (Proposition~\ref{prop:integral_repr}) 
and basic properties of the hinge function. The Lean code is available at 
\url{https://github.com/HenryR2112/option_fitter/tree/main/proof}.

\section{Software Implementation and Practical Demonstration}\label{sec:software}

The unified perspective presented in the previous sections is not merely theoretical. To demonstrate its practical utility and to provide a tool for exploration, we have developed a comprehensive, open-source Python library and application. This software directly embodies the equivalence between option portfolios, spline approximations, and neural networks, enabling practical payoff decomposition, pricing, and risk analysis.

\subsection{System Overview and Design}

The software system is structured around a central \texttt{OptionsFunctionApproximator} class. Its primary function is to solve the regularized least-squares problem associated with Theorem~\ref{thm:density}:
\[
\min_{\mathbf{w}} \sum_{j=1}^{M} \left| f(x_j) - \sum_{i=1}^{N} w_i \, \phi_{K_i}(x_j) \right|^2 + \lambda \|\mathbf{w}\|^2,
\]
where $\phi_{K_i}$ are the selected basis functions (calls, puts, stock position). The system is designed to be extensible, supporting not only the canonical hinge functions but also additional bases like Gaussian kernels or sigmoids, facilitating a comparison of approximation methods.

\subsection{Core Functionality: From Theory to Practice}

\paragraph{Approximation Engine}
The core algorithm takes a target function $f$, a set of strikes $\{K_i\}$ (which can be uniformly spaced or user-defined), and computes the optimal portfolio weights $\mathbf{w}$. This process is the computational realization of the constructive density proof, providing a tangible numerical counterpart to the theoretical approximation guarantees.

\paragraph{Pricing and Risk Analytics}
Moving beyond payoff replication, the system integrates the Black-Scholes model to assign monetary value and risk metrics to the synthesized portfolio. For each option in the portfolio, it calculates:
\begin{itemize}
    \item \textbf{Premium}: The theoretical cost (or credit) for entering the position.
    \item \textbf{Greeks}: First- and second-order sensitivities (Delta, Gamma, Vega, Theta) of the portfolio's value.
\end{itemize}
This transforms a mathematical approximation into a tractable financial instrument, complete with cost analysis and risk exposure reports. The total portfolio cost and its decomposition provide immediate insight into the economic feasibility of a given replication.

\paragraph{Interactive GUI Application}
To make the synthesis accessible, a professional graphical user interface (GUI) is provided (see Figure~\ref{fig:gui_screenshot}). The GUI allows users to:
\begin{itemize}
    \item Select from predefined target functions (e.g., sinusoidal payoffs, option spreads) or define custom ones.
    \item Adjust financial parameters (risk-free rate, volatility, time to expiry) and approximation parameters (number of options, strike range, regularization).
    \item Visualize, in real-time, the quality of the approximation alongside the constituent option payoffs.
    \item Inspect detailed tables of option positions, costs, and aggregated Greek exposures.
\end{itemize}
This interface serves as a direct bridge, allowing practitioners to experiment with the mathematical concepts without writing code.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{image1.png}
    \caption{A view of the interactive GUI for the Options Function Approximator. The main chart shows the target payoff (blue) against the portfolio approximation (orange). Side panels display the portfolio's composition, costs, and risk sensitivities (Greeks).}
    \label{fig:gui_screenshot}
\end{figure}

\subsection{Bridging the Disciplines in Practice}

The software explicitly connects the three fields highlighted in this paper:
\begin{itemize}
    \item \textbf{Finance}: It produces a tradable option portfolio with standard risk metrics.
    \item \textbf{Approximation Theory}: It solves a regularized linear approximation problem, where strike points act as spline knots.
    \item \textbf{Machine Learning}: The approximation engine is, algorithmically, equivalent to training a single-layer ReLU network with fixed biases (strikes) via linear regression. The learned weights correspond directly to option notionals.
\end{itemize}
By outputting both the mathematical approximation error and the financial cost/Greeks, the tool allows users to navigate the inherent trade-off between accuracy and practicality, a central concern in applied financial engineering.

\subsection{Applications and Extensibility}

The primary use cases include \textbf{structured product decomposition} for risk management, \textbf{educational exploration} of payoff replication, and as a \textbf{prototyping tool} for designing custom derivative strategies. The modular architecture allows researchers to easily extend the system with new basis functions or alternative pricing models, making it a platform for further research at the intersection of these fields.

The complete source code, installation instructions, and documentation are available at: \url{https://github.com/HenryR2112/option_fitter}. This implementation serves as both a proof-of-concept for the theoretical synthesis and a practical tool for its application.
\section{Conclusion and Future Directions}\label{sec:conclusion}

\subsection{Summary}

We have presented a unified framework showing that European call option payoffs, linear B-splines, and ReLU activations are mathematically identical as the hinge function $\phi_K(x) = (x-K)_+$. The density of their linear span in $C([a,b])$ is known in each field separately; our contribution is to make the connections explicit and provide a coherent exposition.

\subsection{Limitations}

The theoretical framework assumes:
\begin{itemize}
\item Continuous strike availability (real markets have discrete strikes)
\item No transaction costs or liquidity constraints
\item Single underlying asset (univariate functions)
\item Static portfolios (no time dimension)
\end{itemize}

\subsection{Potential Research Directions}

The connections highlighted here suggest several directions for future investigation:
\begin{enumerate}
\item \textbf{Strike/knot placement}: The problem of optimally placing a limited 
number of strikes $K_i$ to approximate a given function $f$ appears in all three 
fields but with different terminology and techniques.

\item \textbf{Constrained approximation}: Real option markets impose constraints 
(discrete strikes, transaction costs) that could inform constrained versions of 
spline approximation or neural network training.

\item \textbf{Interpretable machine learning}: The equivalence between ReLU networks 
and option portfolios suggests potential approaches to interpreting trained networks 
in financial contexts.
\end{enumerate}

\subsection{Concluding Remarks}

The equivalence $\text{option} = \text{spline} = \text{ReLU}$ reveals a deep mathematical unity across fields that are often studied separately. This suggests that techniques from approximation theory (optimal knot placement) could inform financial engineering (optimal strike selection), while insights from neural network training could improve spline approximation methods.

By making these connections explicit, we hope to facilitate cross-disciplinary research and provide a solid foundation for future work at the intersection of mathematical finance, approximation theory, and machine learning.

\begin{thebibliography}{9}
\bibitem{breeden1978prices}
Breeden, D. T., and Litzenberger, R. H. (1978). Prices of state-contingent claims implicit in option prices. \emph{Journal of Business}, 51(4), 621-651.

\bibitem{carr2001towards}
Carr, P., and Madan, D. (2001). Towards a theory of volatility trading. \emph{Option Pricing, Interest Rates and Risk Management}, 458-476.

\bibitem{cybenko1989approximation}
Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. \emph{Mathematics of Control, Signals and Systems}, 2(4), 303-314.

\bibitem{deboor2001}
de Boor, C. (2001). \emph{A Practical Guide to Splines}. Springer.

\bibitem{leshno1993multilayer}
Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. (1993). Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. \emph{Neural Networks}, 6(6), 861-867.

\bibitem{rudin1976}
Rudin, W. (1976). \emph{Principles of Mathematical Analysis} (3rd ed.). McGraw-Hill.

\bibitem{hornik1989}
Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal approximators. \emph{Neural Networks}, 2(5), 359–366. \doi{10.1016/0893-6080(89)90020-8}. Available at: \url{https://www.sciencedirect.com/science/article/pii/0893608089900208}.
\end{thebibliography}



\end{document}